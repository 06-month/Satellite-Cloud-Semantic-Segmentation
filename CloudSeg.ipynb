{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f0f1f360-76c4-4dc3-9a50-34b96b5a435f",
    "_uuid": "ce5dfba7-6c2e-441d-b852-86e7cc25d385",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Cloud Segmentation - CMX Model (독립 실행 버전)\n",
    "\n",
    "CMX (Cross-Modal Fusion) 모델을 사용한 구름 세그멘테이션\n",
    "\n",
    "## 노트북 구조\n",
    "1. Imports & Config\n",
    "2. Utility Modules (FRM, FFM)\n",
    "3. Decoder\n",
    "4. Backbone\n",
    "5. CMX Model\n",
    "6. Augmentations & Dataset\n",
    "7. Losses & Utils\n",
    "8. Training & Testing\n",
    "9. Main Functions\n",
    "10. 실행\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "82cd01e8-d0a5-4df1-b191-cd63711d2e2b",
    "_uuid": "d96bd0c4-6678-428e-8e51-55a1c70601c5",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:54:26.817779Z",
     "iopub.status.busy": "2025-12-07T06:54:26.817528Z",
     "iopub.status.idle": "2025-12-07T06:54:26.986993Z",
     "shell.execute_reply": "2025-12-07T06:54:26.986095Z",
     "shell.execute_reply.started": "2025-12-07T06:54:26.817760Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 10 08:36:26 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:19:00.0 Off |                  N/A |\n",
      "|  0%   21C    P5              12W / 250W |   1196MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:1A:00.0 Off |                  N/A |\n",
      "|  0%   21C    P8              10W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:67:00.0 Off |                  N/A |\n",
      "|  0%   21C    P8              19W / 250W |      8MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA GeForce GTX 1080 Ti     Off | 00000000:68:00.0 Off |                  N/A |\n",
      "|  0%   22C    P8              19W / 250W |     18MiB / 11264MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "98d6155a-fcc8-41dd-9bc8-08910bd1287b",
    "_uuid": "5bfc1cda-1e65-41e2-98e3-ed8fb18d9736",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 1. Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "bc48eaab-6feb-4989-bc02-c102cacef1c7",
    "_uuid": "6fefe5cb-b7d2-45fd-bc7e-ee3b8c00c341",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:54:26.989270Z",
     "iopub.status.busy": "2025-12-07T06:54:26.988980Z",
     "iopub.status.idle": "2025-12-07T06:54:26.994841Z",
     "shell.execute_reply": "2025-12-07T06:54:26.994037Z",
     "shell.execute_reply.started": "2025-12-07T06:54:26.989238Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: /workspace/basic_code_semina_01/hanbat/CloudSeg\n",
      "Files: ['utils.py', 'segformer_4ch.ipynb', 'main.py', '__pycache__', 'cache', 'output', 'cs-cmx (2).ipynb', 'segformer_4ch_complete.ipynb', 'clouds-segmentation-2025', 'model.py', 'config.py', 'cloudseg.ipynb', 'submission.csv', 'dataset.py', 'generate_submission.py', 'requirements.txt', 'create_submission_baseline.py', 'test_output.log', 'test.py', 'baseline-clouds-segmentation-kaggle-2025 (3).ipynb', 'requirements_p100_install.sh', 'models', 'sample_submission.csv', 'pretrained', 'train.py', 'losses.py', 'test_code_check.py', 'augmentations.py', '123123123.ipynb', 'cs-cmx (3).ipynb', 'cloud_segmentation_complete.py', 'RGBX_Semantic_Segmentation', 'cloud_segmentation_cmx.ipynb', 'merge_files.py']\n",
      "Torch path: None\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "print(\"Files:\", os.listdir())\n",
    "print(\"Torch path:\", sys.modules.get('torch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e016e3dd-476a-462e-8422-fdaf53e81d9d",
    "_uuid": "ccc84952-1dd3-4311-a3fd-7dc129faacc2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:54:26.995754Z",
     "iopub.status.busy": "2025-12-07T06:54:26.995596Z",
     "iopub.status.idle": "2025-12-07T06:55:00.928657Z",
     "shell.execute_reply": "2025-12-07T06:55:00.928051Z",
     "shell.execute_reply.started": "2025-12-07T06:54:26.995741Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/conda/lib/python3.11/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda, Batch: 4, Epochs: 60\n",
      "Augmentation: Copy-Paste=ON\n",
      "Device: cuda, Batch: 4, Epochs: 60\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Cloud Segmentation - CMX Model\n",
    "\"\"\"\n",
    "import os\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import logging\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from timm.models.layers import trunc_normal_, DropPath, to_2tuple\n",
    "from transformers import SegformerModel\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ========== CONFIG ==========\n",
    "workspace_path = '/kaggle/input/clouds-segmentation-2025' \n",
    "output_path = '/kaggle/working'\n",
    "\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "epochs = 60\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "patch_size = 512 \n",
    "patch_stride = 512 // 4 \n",
    "num_workers = 4 \n",
    "num_classes = 4 \n",
    "train_data_rate = 0.8 \n",
    "loss_func = 'ohem+dice' \n",
    "accumulation_steps = 4\n",
    "cmx_backbone = 'mit_b2'\n",
    "cmx_pretrained = True\n",
    "use_timm_pretrained = True\n",
    "\n",
    "lr_head = 3e-4\n",
    "lr_backbone = 3e-5\n",
    "weight_decay = 2e-2 \n",
    "\n",
    "use_scheduler = True\n",
    "scheduler_type = \"cosine\" \n",
    "resume = False \n",
    "seed = 0     \n",
    "use_copy_paste = True \n",
    "\n",
    "print(f\"Device: {device}, Batch: {batch_size}, Epochs: {epochs}\")\n",
    "print(f\"Augmentation: Copy-Paste={'ON' if use_copy_paste else 'OFF'}\")\n",
    "\n",
    "print(f\"Device: {device}, Batch: {batch_size}, Epochs: {epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8affbf31-d791-4900-8eb2-ba2a5a587f07",
    "_uuid": "6abbb4c5-4a71-4a84-ab36-ff4bc87c8e95",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 2. Utility Modules (FRM, FFM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "4426b7a6-d323-4985-be84-efb80ed197be",
    "_uuid": "5ed43ace-0034-484e-9c0a-9e2b8de7c2b4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:00.929922Z",
     "iopub.status.busy": "2025-12-07T06:55:00.929371Z",
     "iopub.status.idle": "2025-12-07T06:55:00.951710Z",
     "shell.execute_reply": "2025-12-07T06:55:00.950938Z",
     "shell.execute_reply.started": "2025-12-07T06:55:00.929902Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRM & FFM defined!\n"
     ]
    }
   ],
   "source": [
    "# ========== Feature Rectify Module (FRM) ==========\n",
    "class ChannelWeights(nn.Module):\n",
    "    def __init__(self, dim, reduction=1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.dim * 4, self.dim * 4 // reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(self.dim * 4 // reduction, self.dim * 2), \n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        B, _, H, W = x1.shape\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        avg = self.avg_pool(x).view(B, self.dim * 2)\n",
    "        max_val = self.max_pool(x).view(B, self.dim * 2)\n",
    "        y = torch.cat((avg, max_val), dim=1)\n",
    "        y = self.mlp(y).view(B, self.dim * 2, 1)\n",
    "        return y.reshape(B, 2, self.dim, 1, 1).permute(1, 0, 2, 3, 4)\n",
    "\n",
    "\n",
    "class SpatialWeights(nn.Module):\n",
    "    def __init__(self, dim, reduction=1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Conv2d(dim * 2, dim // reduction, kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(dim // reduction, 2, kernel_size=1), \n",
    "            nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        B, _, H, W = x1.shape\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        return self.mlp(x).reshape(B, 2, 1, H, W).permute(1, 0, 2, 3, 4)\n",
    "\n",
    "\n",
    "class FeatureRectifyModule(nn.Module):\n",
    "    def __init__(self, dim, reduction=1, lambda_c=.5, lambda_s=.5):\n",
    "        super().__init__()\n",
    "        self.lambda_c, self.lambda_s = lambda_c, lambda_s\n",
    "        self.channel_weights = ChannelWeights(dim=dim, reduction=reduction)\n",
    "        self.spatial_weights = SpatialWeights(dim=dim, reduction=reduction)\n",
    "    \n",
    "    def forward(self, x1, x2):\n",
    "        cw = self.channel_weights(x1, x2)\n",
    "        sw = self.spatial_weights(x1, x2)\n",
    "        out_x1 = x1 + self.lambda_c * cw[1] * x2 + self.lambda_s * sw[1] * x2\n",
    "        out_x2 = x2 + self.lambda_c * cw[0] * x1 + self.lambda_s * sw[0] * x1\n",
    "        return out_x1, out_x2\n",
    "\n",
    "FRM = FeatureRectifyModule\n",
    "\n",
    "\n",
    "# ========== Feature Fusion Module (FFM) ==========\n",
    "class CrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None):\n",
    "        super().__init__()\n",
    "        assert dim % num_heads == 0\n",
    "        self.dim, self.num_heads = dim, num_heads\n",
    "        self.scale = qk_scale or (dim // num_heads) ** -0.5\n",
    "        self.kv1 = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.kv2 = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        B, N, C = x1.shape\n",
    "        q1 = x1.reshape(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        q2 = x2.reshape(B, -1, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        k1, v1 = self.kv1(x1).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        k2, v2 = self.kv2(x2).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        ctx1 = (k1.transpose(-2, -1) @ v1) * self.scale\n",
    "        ctx2 = (k2.transpose(-2, -1) @ v2) * self.scale\n",
    "        x1 = (q1 @ ctx2.softmax(dim=-2)).permute(0, 2, 1, 3).reshape(B, N, C)\n",
    "        x2 = (q2 @ ctx1.softmax(dim=-2)).permute(0, 2, 1, 3).reshape(B, N, C)\n",
    "        return x1, x2\n",
    "\n",
    "\n",
    "class CrossPath(nn.Module):\n",
    "    def __init__(self, dim, reduction=1, num_heads=None, norm_layer=nn.LayerNorm):\n",
    "        super().__init__()\n",
    "        self.channel_proj1 = nn.Linear(dim, dim // reduction * 2)\n",
    "        self.channel_proj2 = nn.Linear(dim, dim // reduction * 2)\n",
    "        self.act1, self.act2 = nn.ReLU(inplace=True), nn.ReLU(inplace=True)\n",
    "        self.cross_attn = CrossAttention(dim // reduction, num_heads=num_heads)\n",
    "        self.end_proj1 = nn.Linear(dim // reduction * 2, dim)\n",
    "        self.end_proj2 = nn.Linear(dim // reduction * 2, dim)\n",
    "        self.norm1, self.norm2 = norm_layer(dim), norm_layer(dim)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        y1, u1 = self.act1(self.channel_proj1(x1)).chunk(2, dim=-1)\n",
    "        y2, u2 = self.act2(self.channel_proj2(x2)).chunk(2, dim=-1)\n",
    "        v1, v2 = self.cross_attn(u1, u2)\n",
    "        return self.norm1(x1 + self.end_proj1(torch.cat((y1, v1), dim=-1))), self.norm2(x2 + self.end_proj2(torch.cat((y2, v2), dim=-1)))\n",
    "\n",
    "\n",
    "class ChannelEmbed(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, reduction=1, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.residual = nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.channel_embed = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels//reduction, 1, bias=True),\n",
    "            nn.Conv2d(out_channels//reduction, out_channels//reduction, 3, 1, 1, bias=True, groups=out_channels//reduction),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels//reduction, out_channels, 1, bias=True),\n",
    "            norm_layer(out_channels))\n",
    "        self.norm = norm_layer(out_channels)\n",
    "        \n",
    "    def forward(self, x, H, W):\n",
    "        B, N, _C = x.shape\n",
    "        x = x.permute(0, 2, 1).reshape(B, _C, H, W)\n",
    "        return self.norm(self.residual(x) + self.channel_embed(x))\n",
    "\n",
    "\n",
    "class FeatureFusionModule(nn.Module):\n",
    "    def __init__(self, dim, reduction=1, num_heads=None, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        self.cross = CrossPath(dim=dim, reduction=reduction, num_heads=num_heads)\n",
    "        self.channel_emb = ChannelEmbed(dim*2, dim, reduction, norm_layer)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        B, C, H, W = x1.shape\n",
    "        x1, x2 = x1.flatten(2).transpose(1, 2), x2.flatten(2).transpose(1, 2)\n",
    "        x1, x2 = self.cross(x1, x2)\n",
    "        return self.channel_emb(torch.cat((x1, x2), dim=-1), H, W)\n",
    "\n",
    "FFM = FeatureFusionModule\n",
    "print(\"FRM & FFM defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b6c52b92-1a72-483d-8cd4-57d14dd5c56d",
    "_uuid": "3ea09f7f-61b2-4cb0-a68c-94cc2915adab",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 3. Decoder (MLPDecoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "63878c37-024f-4676-9999-d06f7cd2f9fb",
    "_uuid": "509c49e5-7484-4579-8cd7-3906a5077bcc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:00.954410Z",
     "iopub.status.busy": "2025-12-07T06:55:00.954152Z",
     "iopub.status.idle": "2025-12-07T06:55:00.984909Z",
     "shell.execute_reply": "2025-12-07T06:55:00.984370Z",
     "shell.execute_reply.started": "2025-12-07T06:55:00.954394Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder defined!\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=2048, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(input_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.proj(x.flatten(2).transpose(1, 2))\n",
    "\n",
    "\n",
    "class DecoderHead(nn.Module):\n",
    "    def __init__(self, in_channels=[64, 128, 320, 512], num_classes=40, dropout_ratio=0.1, \n",
    "                 norm_layer=nn.BatchNorm2d, embed_dim=768, align_corners=False):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.align_corners = align_corners\n",
    "        self.dropout = nn.Dropout2d(dropout_ratio) if dropout_ratio > 0 else None\n",
    "        \n",
    "        c1, c2, c3, c4 = in_channels\n",
    "        self.linear_c4 = MLP(c4, embed_dim)\n",
    "        self.linear_c3 = MLP(c3, embed_dim)\n",
    "        self.linear_c2 = MLP(c2, embed_dim)\n",
    "        self.linear_c1 = MLP(c1, embed_dim)\n",
    "        self.linear_fuse = nn.Sequential(\n",
    "            nn.Conv2d(embed_dim*4, embed_dim, 1), norm_layer(embed_dim), nn.ReLU(inplace=True))\n",
    "        self.linear_pred = nn.Conv2d(embed_dim, num_classes, 1)\n",
    "       \n",
    "    def forward(self, inputs):\n",
    "        c1, c2, c3, c4 = inputs\n",
    "        n, _, h, w = c4.shape\n",
    "        \n",
    "        _c4 = self.linear_c4(c4).permute(0,2,1).reshape(n, -1, c4.shape[2], c4.shape[3])\n",
    "        _c4 = F.interpolate(_c4, size=c1.size()[2:], mode='bilinear', align_corners=self.align_corners)\n",
    "        _c3 = F.interpolate(self.linear_c3(c3).permute(0,2,1).reshape(n, -1, c3.shape[2], c3.shape[3]), size=c1.size()[2:], mode='bilinear', align_corners=self.align_corners)\n",
    "        _c2 = F.interpolate(self.linear_c2(c2).permute(0,2,1).reshape(n, -1, c2.shape[2], c2.shape[3]), size=c1.size()[2:], mode='bilinear', align_corners=self.align_corners)\n",
    "        _c1 = self.linear_c1(c1).permute(0,2,1).reshape(n, -1, c1.shape[2], c1.shape[3])\n",
    "        \n",
    "        _c = self.linear_fuse(torch.cat([_c4, _c3, _c2, _c1], dim=1))\n",
    "        return self.linear_pred(self.dropout(_c) if self.dropout else _c)\n",
    "\n",
    "print(\"Decoder defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3dc3379c-5374-463b-bf50-b66236c53e12",
    "_uuid": "b6eb576a-8072-4e69-901e-c22623081bab",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 4. Backbone (MiT Transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "7e39c88a-3a06-4c38-8b4a-3b6645e853d8",
    "_uuid": "142c2db7-3f2b-49bd-9a4b-6029dc19ac3e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:00.985872Z",
     "iopub.status.busy": "2025-12-07T06:55:00.985558Z",
     "iopub.status.idle": "2025-12-07T06:55:01.004769Z",
     "shell.execute_reply": "2025-12-07T06:55:01.004260Z",
     "shell.execute_reply.started": "2025-12-07T06:55:00.985851Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone components defined!\n"
     ]
    }
   ],
   "source": [
    "class DWConv(nn.Module):\n",
    "    def __init__(self, dim=768):\n",
    "        super().__init__()\n",
    "        self.dwconv = nn.Conv2d(dim, dim, 3, 1, 1, bias=True, groups=dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        return self.dwconv(x.permute(0, 2, 1).reshape(B, C, H, W)).flatten(2).transpose(1, 2)\n",
    "\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.dwconv = DWConv(hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = self.fc1(x)\n",
    "        x = self.dwconv(x, H, W)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.drop(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0., sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.dim, self.num_heads = dim, num_heads\n",
    "        self.scale = qk_scale or (dim // num_heads) ** -0.5\n",
    "        self.q = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.kv = nn.Linear(dim, dim * 2, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.sr_ratio = sr_ratio\n",
    "        if sr_ratio > 1:\n",
    "            self.sr = nn.Conv2d(dim, dim, kernel_size=sr_ratio, stride=sr_ratio)\n",
    "            self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        B, N, C = x.shape\n",
    "        q = self.q(x).reshape(B, N, self.num_heads, C // self.num_heads).permute(0, 2, 1, 3)\n",
    "        if self.sr_ratio > 1:\n",
    "            x_ = self.norm(self.sr(x.permute(0, 2, 1).reshape(B, C, H, W)).reshape(B, C, -1).permute(0, 2, 1))\n",
    "            kv = self.kv(x_).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        else:\n",
    "            kv = self.kv(x).reshape(B, -1, 2, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        k, v = kv[0], kv[1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = self.attn_drop(attn.softmax(dim=-1))\n",
    "        x = self.proj_drop(self.proj((attn @ v).transpose(1, 2).reshape(B, N, C)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., \n",
    "                 attn_drop=0., drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, sr_ratio=1):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop=attn_drop, proj_drop=drop, sr_ratio=sr_ratio)\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=int(dim * mlp_ratio), act_layer=act_layer, drop=drop)\n",
    "\n",
    "    def forward(self, x, H, W):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), H, W))\n",
    "        return x + self.drop_path(self.mlp(self.norm2(x), H, W))\n",
    "\n",
    "\n",
    "class OverlapPatchEmbed(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=7, stride=4, in_chans=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        img_size = to_2tuple(img_size)\n",
    "        patch_size = to_2tuple(patch_size)\n",
    "        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=stride,\n",
    "                              padding=(patch_size[0] // 2, patch_size[1] // 2))\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x)\n",
    "        _, _, H, W = x.shape\n",
    "        return self.norm(x.flatten(2).transpose(1, 2)), H, W\n",
    "\n",
    "print(\"Backbone components defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "6e12d2d7-a7ac-4466-85a3-c36531978c24",
    "_uuid": "b9568a70-732c-4c7b-8b54-ab4ceaee222a",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:01.006668Z",
     "iopub.status.busy": "2025-12-07T06:55:01.005718Z",
     "iopub.status.idle": "2025-12-07T06:55:01.029448Z",
     "shell.execute_reply": "2025-12-07T06:55:01.028748Z",
     "shell.execute_reply.started": "2025-12-07T06:55:01.006643Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RGBXTransformer defined!\n"
     ]
    }
   ],
   "source": [
    "class RGBXTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dims=[64, 128, 256, 512], \n",
    "                 num_heads=[1, 2, 4, 8], mlp_ratios=[4, 4, 4, 4], qkv_bias=False, qk_scale=None, drop_rate=0.,\n",
    "                 attn_drop_rate=0., drop_path_rate=0., norm_layer=nn.LayerNorm, norm_fuse=nn.BatchNorm2d,\n",
    "                 depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1]):\n",
    "        super().__init__()\n",
    "        self.depths = depths\n",
    "        \n",
    "        # RGB patch embeds\n",
    "        self.patch_embed1 = OverlapPatchEmbed(img_size, 7, 4, in_chans, embed_dims[0])\n",
    "        self.patch_embed2 = OverlapPatchEmbed(img_size // 4, 3, 2, embed_dims[0], embed_dims[1])\n",
    "        self.patch_embed3 = OverlapPatchEmbed(img_size // 8, 3, 2, embed_dims[1], embed_dims[2])\n",
    "        self.patch_embed4 = OverlapPatchEmbed(img_size // 16, 3, 2, embed_dims[2], embed_dims[3])\n",
    "        \n",
    "        # NIR patch embeds\n",
    "        self.extra_patch_embed1 = OverlapPatchEmbed(img_size, 7, 4, in_chans, embed_dims[0])\n",
    "        self.extra_patch_embed2 = OverlapPatchEmbed(img_size // 4, 3, 2, embed_dims[0], embed_dims[1])\n",
    "        self.extra_patch_embed3 = OverlapPatchEmbed(img_size // 8, 3, 2, embed_dims[1], embed_dims[2])\n",
    "        self.extra_patch_embed4 = OverlapPatchEmbed(img_size // 16, 3, 2, embed_dims[2], embed_dims[3])\n",
    "        \n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        cur = 0\n",
    "        \n",
    "        # Stage 1\n",
    "        self.block1 = nn.ModuleList([Block(embed_dims[0], num_heads[0], mlp_ratios[0], qkv_bias, qk_scale,\n",
    "            drop_rate, attn_drop_rate, dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[0]) for i in range(depths[0])])\n",
    "        self.norm1 = norm_layer(embed_dims[0])\n",
    "        self.extra_block1 = nn.ModuleList([Block(embed_dims[0], num_heads[0], mlp_ratios[0], qkv_bias, qk_scale,\n",
    "            drop_rate, attn_drop_rate, dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[0]) for i in range(depths[0])])\n",
    "        self.extra_norm1 = norm_layer(embed_dims[0])\n",
    "        cur += depths[0]\n",
    "        \n",
    "        # Stage 2\n",
    "        self.block2 = nn.ModuleList([Block(embed_dims[1], num_heads[1], mlp_ratios[1], qkv_bias, qk_scale,\n",
    "            drop_rate, attn_drop_rate, dpr[cur], norm_layer=norm_layer, sr_ratio=sr_ratios[1]) for i in range(depths[1])])\n",
    "        self.norm2 = norm_layer(embed_dims[1])\n",
    "        self.extra_block2 = nn.ModuleList([Block(embed_dims[1], num_heads[1], mlp_ratios[1], qkv_bias, qk_scale,\n",
    "            drop_rate, attn_drop_rate, dpr[cur+1], norm_layer=norm_layer, sr_ratio=sr_ratios[1]) for i in range(depths[1])])\n",
    "        self.extra_norm2 = norm_layer(embed_dims[1])\n",
    "        cur += depths[1]\n",
    "        \n",
    "        # Stage 3\n",
    "        self.block3 = nn.ModuleList([Block(embed_dims[2], num_heads[2], mlp_ratios[2], qkv_bias, qk_scale,\n",
    "            drop_rate, attn_drop_rate, dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[2]) for i in range(depths[2])])\n",
    "        self.norm3 = norm_layer(embed_dims[2])\n",
    "        self.extra_block3 = nn.ModuleList([Block(embed_dims[2], num_heads[2], mlp_ratios[2], qkv_bias, qk_scale,\n",
    "            drop_rate, attn_drop_rate, dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[2]) for i in range(depths[2])])\n",
    "        self.extra_norm3 = norm_layer(embed_dims[2])\n",
    "        cur += depths[2]\n",
    "        \n",
    "        # Stage 4\n",
    "        self.block4 = nn.ModuleList([Block(embed_dims[3], num_heads[3], mlp_ratios[3], qkv_bias, qk_scale,\n",
    "            drop_rate, attn_drop_rate, dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[3]) for i in range(depths[3])])\n",
    "        self.norm4 = norm_layer(embed_dims[3])\n",
    "        self.extra_block4 = nn.ModuleList([Block(embed_dims[3], num_heads[3], mlp_ratios[3], qkv_bias, qk_scale,\n",
    "            drop_rate, attn_drop_rate, dpr[cur + i], norm_layer=norm_layer, sr_ratio=sr_ratios[3]) for i in range(depths[3])])\n",
    "        self.extra_norm4 = norm_layer(embed_dims[3])\n",
    "        \n",
    "        # FRM & FFM\n",
    "        self.FRMs = nn.ModuleList([FRM(dim=embed_dims[i], reduction=1) for i in range(4)])\n",
    "        self.FFMs = nn.ModuleList([FFM(dim=embed_dims[i], reduction=1, num_heads=num_heads[i], norm_layer=norm_fuse) for i in range(4)])\n",
    "\n",
    "    def forward_features(self, x_rgb, x_e):\n",
    "        B = x_rgb.shape[0]\n",
    "        outs = []\n",
    "        \n",
    "        # Stage 1\n",
    "        x_rgb, H, W = self.patch_embed1(x_rgb)\n",
    "        x_e, _, _ = self.extra_patch_embed1(x_e)\n",
    "        for blk in self.block1: x_rgb = blk(x_rgb, H, W)\n",
    "        for blk in self.extra_block1: x_e = blk(x_e, H, W)\n",
    "        x_rgb, x_e = self.norm1(x_rgb), self.extra_norm1(x_e)\n",
    "        x_rgb = x_rgb.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x_e = x_e.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x_rgb, x_e = self.FRMs[0](x_rgb, x_e)\n",
    "        outs.append(self.FFMs[0](x_rgb, x_e))\n",
    "        \n",
    "        # Stage 2\n",
    "        x_rgb, H, W = self.patch_embed2(x_rgb)\n",
    "        x_e, _, _ = self.extra_patch_embed2(x_e)\n",
    "        for blk in self.block2: x_rgb = blk(x_rgb, H, W)\n",
    "        for blk in self.extra_block2: x_e = blk(x_e, H, W)\n",
    "        x_rgb, x_e = self.norm2(x_rgb), self.extra_norm2(x_e)\n",
    "        x_rgb = x_rgb.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x_e = x_e.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x_rgb, x_e = self.FRMs[1](x_rgb, x_e)\n",
    "        outs.append(self.FFMs[1](x_rgb, x_e))\n",
    "        \n",
    "        # Stage 3\n",
    "        x_rgb, H, W = self.patch_embed3(x_rgb)\n",
    "        x_e, _, _ = self.extra_patch_embed3(x_e)\n",
    "        for blk in self.block3: x_rgb = blk(x_rgb, H, W)\n",
    "        for blk in self.extra_block3: x_e = blk(x_e, H, W)\n",
    "        x_rgb, x_e = self.norm3(x_rgb), self.extra_norm3(x_e)\n",
    "        x_rgb = x_rgb.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x_e = x_e.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x_rgb, x_e = self.FRMs[2](x_rgb, x_e)\n",
    "        outs.append(self.FFMs[2](x_rgb, x_e))\n",
    "        \n",
    "        # Stage 4\n",
    "        x_rgb, H, W = self.patch_embed4(x_rgb)\n",
    "        x_e, _, _ = self.extra_patch_embed4(x_e)\n",
    "        for blk in self.block4: x_rgb = blk(x_rgb, H, W)\n",
    "        for blk in self.extra_block4: x_e = blk(x_e, H, W)\n",
    "        x_rgb, x_e = self.norm4(x_rgb), self.extra_norm4(x_e)\n",
    "        x_rgb = x_rgb.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x_e = x_e.reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "        x_rgb, x_e = self.FRMs[3](x_rgb, x_e)\n",
    "        outs.append(self.FFMs[3](x_rgb, x_e))\n",
    "        \n",
    "        return outs\n",
    "\n",
    "    def forward(self, x_rgb, x_e):\n",
    "        return self.forward_features(x_rgb, x_e)\n",
    "\n",
    "print(\"RGBXTransformer defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "9caca863-ebb6-4b10-ad4d-8290c4b75a22",
    "_uuid": "c3f45713-c3e8-454b-9c47-3768af2b8e54",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:01.030255Z",
     "iopub.status.busy": "2025-12-07T06:55:01.030047Z",
     "iopub.status.idle": "2025-12-07T06:55:01.046357Z",
     "shell.execute_reply": "2025-12-07T06:55:01.045574Z",
     "shell.execute_reply.started": "2025-12-07T06:55:01.030238Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiT variants & pretrained loader defined!\n"
     ]
    }
   ],
   "source": [
    "# MiT Backbone Variants\n",
    "class mit_b1(RGBXTransformer):\n",
    "    def __init__(self, fuse_cfg=None, **kwargs):\n",
    "        super().__init__(patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
    "            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[2, 2, 2, 2], sr_ratios=[8, 4, 2, 1],\n",
    "            drop_rate=0.0, drop_path_rate=0.1)\n",
    "\n",
    "class mit_b2(RGBXTransformer):\n",
    "    def __init__(self, fuse_cfg=None, **kwargs):\n",
    "        super().__init__(patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
    "            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 6, 3], sr_ratios=[8, 4, 2, 1],\n",
    "            drop_rate=0.0, drop_path_rate=0.1)\n",
    "\n",
    "class mit_b3(RGBXTransformer):\n",
    "    def __init__(self, fuse_cfg=None, **kwargs):\n",
    "        super().__init__(patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
    "            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 4, 18, 3], sr_ratios=[8, 4, 2, 1],\n",
    "            drop_rate=0.0, drop_path_rate=0.1)\n",
    "\n",
    "class mit_b4(RGBXTransformer):\n",
    "    def __init__(self, fuse_cfg=None, **kwargs):\n",
    "        super().__init__(patch_size=4, embed_dims=[64, 128, 320, 512], num_heads=[1, 2, 5, 8], mlp_ratios=[4, 4, 4, 4],\n",
    "            qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6), depths=[3, 8, 27, 3], sr_ratios=[8, 4, 2, 1],\n",
    "            drop_rate=0.0, drop_path_rate=0.1)\n",
    "\n",
    "\n",
    "def load_pretrained_from_transformers(model, backbone='mit_b2'):\n",
    "    \"\"\"Hugging Face transformers에서 SegFormer pretrained 가중치 로드\"\"\"\n",
    "    hf_names = {'mit_b1': 'nvidia/mit-b1', 'mit_b2': 'nvidia/mit-b2', 'mit_b3': 'nvidia/mit-b3', 'mit_b4': 'nvidia/mit-b4'}\n",
    "    hf_name = hf_names.get(backbone, 'nvidia/mit-b2')\n",
    "    print(f\"Loading pretrained weights from: {hf_name}...\")\n",
    "    \n",
    "    try:\n",
    "        hf_model = SegformerModel.from_pretrained(hf_name)\n",
    "        raw_state_dict = hf_model.state_dict()\n",
    "        state_dict = {}\n",
    "        kv_weights, kv_biases = {}, {}\n",
    "        \n",
    "        for k, v in raw_state_dict.items():\n",
    "            new_key = k.replace('encoder.', '')\n",
    "            for i in range(4):\n",
    "                new_key = new_key.replace(f'patch_embeddings.{i}', f'patch_embed{i+1}')\n",
    "                new_key = new_key.replace(f'block.{i}.', f'block{i+1}.')\n",
    "                new_key = new_key.replace(f'layer_norm.{i}', f'norm{i+1}')\n",
    "            new_key = new_key.replace('attention.self.query', 'attn.q')\n",
    "            new_key = new_key.replace('attention.output.dense', 'attn.proj')\n",
    "            new_key = new_key.replace('attention.self.sr', 'attn.sr')\n",
    "            new_key = new_key.replace('attention.self.layer_norm', 'attn.norm')\n",
    "            new_key = new_key.replace('output.dense', 'mlp.fc2')\n",
    "            new_key = new_key.replace('intermediate.dense', 'mlp.fc1')\n",
    "            new_key = new_key.replace('dwconv.dwconv', 'mlp.dwconv.dwconv')\n",
    "            \n",
    "            if 'attention.self.key' in k:\n",
    "                base = new_key.replace('attention.self.key', 'attn.kv')\n",
    "                kv_weights.setdefault(base, {})['key'] = v\n",
    "                continue\n",
    "            elif 'attention.self.value' in k:\n",
    "                base = new_key.replace('attention.self.value', 'attn.kv')\n",
    "                kv_weights.setdefault(base, {})['value'] = v\n",
    "                continue\n",
    "            \n",
    "            if any(x in new_key for x in ['patch_embed', 'block', 'norm']):\n",
    "                state_dict[new_key] = v\n",
    "                if 'patch_embed' in new_key:\n",
    "                    state_dict[new_key.replace('patch_embed', 'extra_patch_embed')] = v\n",
    "                elif 'block' in new_key:\n",
    "                    state_dict[new_key.replace('block', 'extra_block')] = v\n",
    "                elif 'norm' in new_key and 'attn' not in new_key:\n",
    "                    state_dict[new_key.replace('norm', 'extra_norm')] = v\n",
    "        \n",
    "        for base, kv in kv_weights.items():\n",
    "            if 'key' in kv and 'value' in kv:\n",
    "                w = torch.cat([kv['key'], kv['value']], dim=0)\n",
    "                state_dict[base] = w\n",
    "                state_dict[base.replace('block', 'extra_block')] = w\n",
    "        \n",
    "        model_state = model.state_dict()\n",
    "        loaded = 0\n",
    "        for k, v in state_dict.items():\n",
    "            if k in model_state and model_state[k].shape == v.shape:\n",
    "                model_state[k] = v\n",
    "                loaded += 1\n",
    "        model.load_state_dict(model_state, strict=False)\n",
    "        print(f\"✓ Loaded {loaded} keys from pretrained!\")\n",
    "        del hf_model\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load pretrained: {e}\")\n",
    "\n",
    "print(\"MiT variants & pretrained loader defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3f8bcd6a-ae97-4a0f-be94-ad737fda28f6",
    "_uuid": "b73597d7-88b6-48ed-915a-f67db5fe67b0",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 5. CMX Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "3fd752bb-bf2f-4b84-88bd-4bb7089816a4",
    "_uuid": "8ac2a154-dce7-45ab-89c3-34a74cfcc2ff",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:01.047292Z",
     "iopub.status.busy": "2025-12-07T06:55:01.047116Z",
     "iopub.status.idle": "2025-12-07T06:55:01.058915Z",
     "shell.execute_reply": "2025-12-07T06:55:01.058322Z",
     "shell.execute_reply.started": "2025-12-07T06:55:01.047278Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMX Model defined!\n"
     ]
    }
   ],
   "source": [
    "class CMXModel(nn.Module):\n",
    "    \"\"\"CMX: Cross-Modal Fusion for RGB-X Semantic Segmentation\"\"\"\n",
    "    def __init__(self, backbone='mit_b2', num_classes=4, decoder_embed_dim=512, \n",
    "                 pretrained_model=None, norm_layer=nn.BatchNorm2d):\n",
    "        super().__init__()\n",
    "        backbones = {'mit_b1': mit_b1, 'mit_b2': mit_b2, 'mit_b3': mit_b3, 'mit_b4': mit_b4}\n",
    "        self.backbone = backbones.get(backbone, mit_b2)(norm_fuse=norm_layer)\n",
    "        channels = [64, 128, 320, 512]  # mit_b1, mit_b2, mit_b3, mit_b4 모두 동일한 채널 수\n",
    "        self.decode_head = DecoderHead(in_channels=channels, num_classes=num_classes, \n",
    "                                        norm_layer=norm_layer, embed_dim=decoder_embed_dim)\n",
    "    \n",
    "    def forward(self, rgb, nir):\n",
    "        if nir.shape[1] == 1:\n",
    "            nir = nir.repeat(1, 3, 1, 1)\n",
    "        features = self.backbone(rgb, nir)\n",
    "        return self.decode_head(features)\n",
    "\n",
    "\n",
    "def create_model(device, num_classes=4, cmx_backbone='mit_b2', cmx_pretrained=None, use_timm_pretrained=True):\n",
    "    model = CMXModel(backbone=cmx_backbone, num_classes=num_classes)\n",
    "    if use_timm_pretrained and cmx_pretrained is None:\n",
    "        load_pretrained_from_transformers(model.backbone, cmx_backbone)\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"CMX Model defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a44367c0-7cdc-4370-afdf-2d9e9b267269",
    "_uuid": "c2d5b116-451d-48bb-bb1b-794a47d7db44",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 6. Augmentations & Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b2fa21df-863e-4ff0-b32a-cb5c6a4a8838",
    "_uuid": "0cafc998-ec23-45bc-ae8d-1cd3ea310c06",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:01.059895Z",
     "iopub.status.busy": "2025-12-07T06:55:01.059629Z",
     "iopub.status.idle": "2025-12-07T06:55:01.089120Z",
     "shell.execute_reply": "2025-12-07T06:55:01.088529Z",
     "shell.execute_reply.started": "2025-12-07T06:55:01.059874Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset updated for Test Support!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =========================================================\n",
    "#  1. Copy-Paste Augmentation (완벽함)\n",
    "# =========================================================\n",
    "class CopyPasteAugmentation:\n",
    "    def __init__(self, p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, rgb, nir, mask, src_rgb, src_nir, src_mask):\n",
    "        if random.random() > self.p:\n",
    "            return rgb, nir, mask\n",
    "\n",
    "        H, W = rgb.shape[:2]\n",
    "\n",
    "        # binary cloud\n",
    "        cloud = (src_mask > 0).astype(np.uint8)\n",
    "        if cloud.sum() == 0:\n",
    "            return rgb, nir, mask\n",
    "\n",
    "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(cloud, connectivity=8)\n",
    "        if num_labels <= 1:\n",
    "            return rgb, nir, mask\n",
    "\n",
    "        # pick random component\n",
    "        comp_idx = random.randint(1, num_labels - 1)\n",
    "        x, y, w, h, area = stats[comp_idx]\n",
    "        if area < 100:\n",
    "            return rgb, nir, mask\n",
    "\n",
    "        comp = (labels == comp_idx).astype(np.uint8)\n",
    "        crop_rgb = src_rgb[y:y+h, x:x+w].copy()\n",
    "        crop_nir = src_nir[y:y+h, x:x+w].copy()\n",
    "        crop_mask = src_mask[y:y+h, x:x+w].copy()\n",
    "        crop_comp = comp[y:y+h, x:x+w].copy()\n",
    "\n",
    "        # resize\n",
    "        scale = random.uniform(0.4, 1.2)\n",
    "        nh, nw = int(h * scale), int(w * scale)\n",
    "        if nh < 2 or nw < 2:\n",
    "            return rgb, nir, mask\n",
    "        \n",
    "        # 크기가 원본 이미지를 초과하지 않도록 제한\n",
    "        nh = min(nh, H)\n",
    "        nw = min(nw, W)\n",
    "\n",
    "        crop_rgb = cv2.resize(crop_rgb, (nw, nh))\n",
    "        crop_nir = cv2.resize(crop_nir, (nw, nh))\n",
    "        crop_mask = cv2.resize(crop_mask, (nw, nh), interpolation=cv2.INTER_NEAREST)\n",
    "        crop_comp = cv2.resize(crop_comp, (nw, nh), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        # ensure shape (H,W,1)\n",
    "        if crop_nir.ndim == 2:\n",
    "            crop_nir = crop_nir[..., None]\n",
    "\n",
    "        # 안전한 위치 계산\n",
    "        px = random.randint(0, max(0, W - nw))\n",
    "        py = random.randint(0, max(0, H - nh))\n",
    "        \n",
    "        # 실제 붙여넣을 영역 크기 계산 (경계 체크)\n",
    "        paste_h = min(nh, H - py)\n",
    "        paste_w = min(nw, W - px)\n",
    "        \n",
    "        # crop도 같은 크기로 자르기\n",
    "        crop_rgb = crop_rgb[:paste_h, :paste_w]\n",
    "        crop_nir = crop_nir[:paste_h, :paste_w]\n",
    "        crop_mask = crop_mask[:paste_h, :paste_w]\n",
    "        crop_comp = crop_comp[:paste_h, :paste_w]\n",
    "        \n",
    "        region = crop_comp.astype(bool)\n",
    "\n",
    "        # apply\n",
    "        rgb_patch = rgb[py:py+paste_h, px:px+paste_w]\n",
    "        nir_patch = nir[py:py+paste_h, px:px+paste_w]\n",
    "        mask_patch = mask[py:py+paste_h, px:px+paste_w]\n",
    "\n",
    "        rgb_patch[region] = crop_rgb[region]\n",
    "        nir_patch[region] = crop_nir[region]\n",
    "        mask_patch[region] = crop_mask[region]\n",
    "\n",
    "        return rgb, nir, mask\n",
    "# =========================================================\n",
    "#  2. Train Transforms (완벽함)\n",
    "# =========================================================\n",
    "class ImageAug:\n",
    "    def __init__(self, crop_size=512):\n",
    "        # 1. 기하학적 변환 (4채널 지원) - RGB+NIR 동시 적용\n",
    "        self.geom = A.Compose([\n",
    "            A.RandomCrop(height=crop_size, width=crop_size, p=1.0),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.5),\n",
    "            A.ShiftScaleRotate(\n",
    "                shift_limit=0.05, \n",
    "                scale_limit=0.2, \n",
    "                rotate_limit=30,\n",
    "                border_mode=0,\n",
    "                p=0.5\n",
    "            ),\n",
    "            A.OneOf([\n",
    "                A.GridDistortion(p=0.5),\n",
    "                A.OpticalDistortion(distort_limit=0.5, p=0.5),\n",
    "            ], p=0.3),\n",
    "        ])\n",
    "        \n",
    "        # 2. 색상 변환 (3채널만 지원) - RGB에만 적용\n",
    "        self.color = A.Compose([\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "            A.HueSaturationValue(hue_shift_limit=15, sat_shift_limit=20, val_shift_limit=15, p=0.5),\n",
    "            A.CLAHE(clip_limit=2.0, p=0.3),\n",
    "        ])\n",
    "\n",
    "        # 3. 정규화 (RGB용)\n",
    "        self.normalize_rgb = A.Compose([\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "        # 4. 정규화 (NIR용)\n",
    "        self.normalize_nir = A.Compose([\n",
    "            A.Normalize(\n",
    "                mean=[0.5],\n",
    "                std=[0.25],\n",
    "                max_pixel_value=255.0\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __call__(self, rgb, nir, mask):\n",
    "        if nir.ndim == 2:\n",
    "            nir = nir[..., None]\n",
    "\n",
    "        # Step 1: 기하학적 변환 (RGB+NIR+Mask 동시 적용)\n",
    "        combined = np.concatenate([rgb, nir], axis=-1)\n",
    "        aug = self.geom(image=combined, mask=mask)\n",
    "        combined = aug[\"image\"]\n",
    "        mask = aug[\"mask\"]\n",
    "        \n",
    "        # 분리\n",
    "        rgb_aug = combined[:, :, :3]\n",
    "        nir_aug = combined[:, :, 3:]\n",
    "\n",
    "        # Step 2: RGB에만 색상 변환 적용\n",
    "        rgb_aug = self.color(image=rgb_aug)[\"image\"]\n",
    "\n",
    "        # Step 3: 정규화 및 텐서 변환\n",
    "        rgb_t = self.normalize_rgb(image=rgb_aug)[\"image\"]\n",
    "        nir_t = self.normalize_nir(image=nir_aug)[\"image\"]\n",
    "        mask_t = torch.from_numpy(mask).long()\n",
    "\n",
    "        return rgb_t, nir_t, mask_t\n",
    "\n",
    "class DefaultAug:\n",
    "    def __init__(self):\n",
    "        self.tf = A.Compose([\n",
    "            A.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406, 0.5],\n",
    "                std=[0.229, 0.224, 0.225, 0.25],\n",
    "                max_pixel_value=255.0\n",
    "            ),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __call__(self, rgb, nir, mask):\n",
    "        if nir.ndim == 2:\n",
    "            nir = nir[..., None]\n",
    "\n",
    "        combined = np.concatenate([rgb, nir], axis=-1)\n",
    "\n",
    "        aug = self.tf(image=combined, mask=mask)\n",
    "        img4 = aug[\"image\"]\n",
    "        mask = aug[\"mask\"]\n",
    "\n",
    "        rgb_t = img4[:3, :, :]\n",
    "        nir_t = img4[3:4, :, :]\n",
    "        mask_t = mask.long()\n",
    "\n",
    "        return rgb_t, nir_t, mask_t\n",
    "# =========================================================\n",
    "#  4. Dataset\n",
    "# =========================================================\n",
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, rgb_paths, ngr_paths, label_paths, \n",
    "                 is_train=True, crop_size=512, use_copy_paste=False):\n",
    "\n",
    "        self.is_train = is_train\n",
    "        self.use_copy_paste = use_copy_paste\n",
    "        self.copy_paste = CopyPasteAugmentation()\n",
    "        \n",
    "        # [수정] 나중에 파일명을 알기 위해 경로 저장\n",
    "        self.rgb_paths_list = rgb_paths\n",
    "\n",
    "        # 원본 이미지 로드 (RAM에 올림)\n",
    "        self.rgb_imgs = [cv2.cvtColor(cv2.imread(p), cv2.COLOR_BGR2RGB) for p in rgb_paths]\n",
    "        self.nir_imgs = [cv2.imread(p)[:, :, 2] for p in ngr_paths]\n",
    "        \n",
    "        # 라벨이 있는 경우에만 로드 (Test셋은 라벨이 없음)\n",
    "        if len(label_paths) > 0:\n",
    "            self.lbl_imgs = [cv2.imread(p) for p in label_paths]\n",
    "            self.has_label = True\n",
    "        else:\n",
    "            self.lbl_imgs = []\n",
    "            self.has_label = False\n",
    "\n",
    "        self.train_tf = ImageAug(crop_size)\n",
    "        self.val_tf = DefaultAug()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rgb_imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 1. 이미지 가져오기\n",
    "        rgb = self.rgb_imgs[idx].copy()\n",
    "        nir = self.nir_imgs[idx].copy()[..., None]  # (H, W) -> (H, W, 1)\n",
    "        if not self.has_label:\n",
    "            dummy = np.zeros(rgb.shape[:2], dtype=np.uint8)\n",
    "            rgb_t, nir_t, _ = self.val_tf(rgb, nir, dummy)\n",
    "            return (rgb_t, nir_t), self.rgb_paths_list[idx]\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # Case A: 학습/검증 (라벨 있음)\n",
    "        # -----------------------------------------------\n",
    "        if self.has_label:\n",
    "            lbl = self.lbl_imgs[idx].copy()\n",
    "            \n",
    "            # 마스크 생성\n",
    "            mask = np.zeros(lbl.shape[:2], dtype=np.uint8)\n",
    "            mask[np.all(lbl == [0,0,255], axis=-1)] = 1  # Thick\n",
    "            mask[np.all(lbl == [0,255,0], axis=-1)] = 2  # Thin\n",
    "            mask[np.all(lbl == [0,255,255], axis=-1)] = 3 # Shadow\n",
    "\n",
    "            # Copy-Paste\n",
    "            if self.is_train and self.use_copy_paste:\n",
    "                j = random.randint(0, len(self.rgb_imgs)-1)\n",
    "                src_rgb = self.rgb_imgs[j]\n",
    "                src_nir = self.nir_imgs[j][..., None]\n",
    "                # 소스 이미지의 라벨도 가져와야 함\n",
    "                src_lbl = self.lbl_imgs[j]\n",
    "                src_mask = np.zeros(src_lbl.shape[:2], dtype=np.uint8)\n",
    "                src_mask[np.all(src_lbl == [0,0,255], axis=-1)] = 1\n",
    "                src_mask[np.all(src_lbl == [0,255,0], axis=-1)] = 2\n",
    "                src_mask[np.all(src_lbl == [0,255,255], axis=-1)] = 3\n",
    "                \n",
    "                rgb, nir, mask = self.copy_paste(rgb, nir, mask, src_rgb, src_nir, src_mask)\n",
    "\n",
    "            # Transform\n",
    "            if self.is_train:\n",
    "                rgb_t, nir_t, mask_t = self.train_tf(rgb, nir, mask)\n",
    "            else:\n",
    "                rgb_t, nir_t, mask_t = self.val_tf(rgb, nir, mask)\n",
    "            \n",
    "            return (rgb_t, nir_t), mask_t\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # Case B: 테스트 (라벨 없음)\n",
    "        # -----------------------------------------------\n",
    "        else:\n",
    "            # 테스트용 Transform (기하학적 변환 없이 정규화만)\n",
    "            # 마스크가 없으므로 더미 마스크(0)를 만들어서 넘김\n",
    "            dummy_mask = np.zeros(rgb.shape[:2], dtype=np.uint8)\n",
    "            rgb_t, nir_t, _ = self.val_tf(rgb, nir, dummy_mask)\n",
    "            \n",
    "            # [중요] 마스크 대신 '파일 경로'를 반환\n",
    "            return (rgb_t, nir_t), self.rgb_paths_list[idx]\n",
    "\n",
    "print(\"Dataset updated for Test Support!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b7aa8682-14e8-475c-aef0-fccc31899c25",
    "_uuid": "a1a21528-d53c-45ed-afa2-b9a27b4272c9",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 7. Losses & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "be1c5ba1-bab0-4f0c-8ef0-f9f3e5d8b2aa",
    "_uuid": "28608436-78fb-4b32-bbd3-f5df0a75d023",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:01.090201Z",
     "iopub.status.busy": "2025-12-07T06:55:01.089915Z",
     "iopub.status.idle": "2025-12-07T06:55:01.109834Z",
     "shell.execute_reply": "2025-12-07T06:55:01.109101Z",
     "shell.execute_reply.started": "2025-12-07T06:55:01.090180Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses & Utils defined (Fixed Version)!\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 1. Loss Functions (Type & Dimension Safe)\n",
    "# =========================================================\n",
    "\n",
    "def ohem_ce_loss(preds, targets, ratio=0.25, ignore_index=255):\n",
    "    targets = targets.long()\n",
    "    if targets.dim() == 4:\n",
    "        targets = targets.squeeze(1)\n",
    "\n",
    "    n, c, h, w = preds.shape\n",
    "    preds = preds.permute(0,2,3,1).reshape(-1, c)\n",
    "    targets = targets.view(-1)\n",
    "\n",
    "    ce = F.cross_entropy(preds, targets, ignore_index=ignore_index, reduction='none')\n",
    "    ce_sorted, idx = torch.sort(ce, descending=True)\n",
    "\n",
    "    num_hard = int(ce.numel() * ratio)\n",
    "    ce_hard = ce_sorted[:num_hard]\n",
    "\n",
    "    return ce_hard.mean()\n",
    "    \n",
    "def dice_loss(preds, targets, eps=1e-7):\n",
    "    \"\"\"\n",
    "    Dice Loss\n",
    "    preds: (B, C, H, W) - Logits\n",
    "    targets: (B, H, W) or (B, 1, H, W) - Indices\n",
    "    \"\"\"\n",
    "    # 1. Targets 안전 처리\n",
    "    targets = targets.long()\n",
    "    if targets.dim() == 4:\n",
    "        targets = targets.squeeze(1) # (B, 1, H, W) -> (B, H, W)\n",
    "    \n",
    "    num_classes = preds.shape[1]\n",
    "    \n",
    "    # 2. One-hot Encoding\n",
    "    # targets: (B, H, W) -> one_hot: (B, H, W, C) -> permute: (B, C, H, W)\n",
    "    true_1_hot = F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    # 3. Softmax\n",
    "    probas = F.softmax(preds, dim=1)\n",
    "    \n",
    "    # 4. Calc Dice\n",
    "    dims = (0,) + tuple(range(2, targets.ndimension() + 1))\n",
    "    \n",
    "    intersection = torch.sum(probas * true_1_hot, dims)\n",
    "    cardinality = torch.sum(probas + true_1_hot, dims)\n",
    "    \n",
    "    return 1 - (2. * intersection / (cardinality + eps)).mean()\n",
    "\n",
    "\n",
    "def jaccard_loss(preds, targets, eps=1e-7):\n",
    "    \"\"\"Jaccard (IoU) Loss\"\"\"\n",
    "    targets = targets.long()\n",
    "    if targets.dim() == 4:\n",
    "        targets = targets.squeeze(1)\n",
    "        \n",
    "    num_classes = preds.shape[1]\n",
    "    true_1_hot = F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float()\n",
    "    probas = F.softmax(preds, dim=1)\n",
    "    \n",
    "    dims = (0,) + tuple(range(2, targets.ndimension() + 1))\n",
    "    \n",
    "    intersection = torch.sum(probas * true_1_hot, dims)\n",
    "    cardinality = torch.sum(probas + true_1_hot, dims)\n",
    "    \n",
    "    return 1 - (intersection / (cardinality - intersection + eps)).mean()\n",
    "\n",
    "\n",
    "def ce_loss(preds, targets, ignore=255):\n",
    "    \"\"\"Cross-Entropy Loss\"\"\"\n",
    "    targets = targets.long()\n",
    "    if targets.dim() == 4:\n",
    "        targets = targets.squeeze(1)\n",
    "        \n",
    "    return F.cross_entropy(preds, targets, ignore_index=ignore)\n",
    "\n",
    "\n",
    "def get_loss_function(name):\n",
    "    \"\"\"Loss 함수 이름으로 함수 반환 (인자 순서 통일됨)\"\"\"\n",
    "    if name == 'dice':\n",
    "        return dice_loss\n",
    "    elif name == 'jaccard':\n",
    "        return jaccard_loss\n",
    "    elif name == 'ce':\n",
    "        return ce_loss\n",
    "    elif name in ['dice+ce']:\n",
    "        def loss(preds, targets):\n",
    "            return 0.7 * ce_loss(preds, targets) + 0.3 * dice_loss(preds, targets)\n",
    "        return loss\n",
    "    elif name in ['dice+jaccard', 'dice + jaccard']:\n",
    "        def combined_loss(preds, targets):\n",
    "            return dice_loss(preds, targets) + jaccard_loss(preds, targets)\n",
    "        return combined_loss\n",
    "    elif name in ['ohem+dice']:\n",
    "        def loss(preds, targets):\n",
    "            return 0.7 * ohem_ce_loss(preds, targets, ratio=0.25) + \\\n",
    "                   0.3 * dice_loss(preds, targets)\n",
    "        return loss\n",
    "    else:\n",
    "        print(f\"Warning: Unknown loss name '{name}'. Using Dice Loss.\")\n",
    "        return dice_loss\n",
    "\n",
    "# =========================================================\n",
    "# 2. Utils & Metrics\n",
    "# =========================================================\n",
    "\n",
    "def init_seeds(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    cudnn.deterministic = True\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def fitness_test(true, pred, num_classes=4):\n",
    "    \"\"\"mIOU, Pixel Accuracy, Dice Score 계산\"\"\"\n",
    "    eps = 1e-7\n",
    "    \n",
    "    # [수정] 차원 및 타입 안전 처리\n",
    "    true = true.long()\n",
    "    if true.dim() == 4:\n",
    "        true = true.squeeze(1)\n",
    "        \n",
    "    # Pred: (B, C, H, W) -> (B, H, W) (argmax)\n",
    "    if pred.shape[1] > 1: # Logits이 들어오면 argmax\n",
    "        pred_max = pred.argmax(dim=1)\n",
    "    else:\n",
    "        pred_max = pred.squeeze(1) # 이미 argmax된 상태라면\n",
    "    \n",
    "    # Pixel Accuracy\n",
    "    pix_acc = (true == pred_max).float().mean()\n",
    "    \n",
    "    # One-hot 변환 for IoU/Dice calculation\n",
    "    true_1h = F.one_hot(true, num_classes).permute(0, 3, 1, 2).float()\n",
    "    pred_1h = F.one_hot(pred_max, num_classes).permute(0, 3, 1, 2).float()\n",
    "    \n",
    "    dims = (0,) + tuple(range(2, true.ndimension() + 1))\n",
    "    \n",
    "    # Intersection & Union\n",
    "    inter = torch.sum(pred_1h * true_1h, dims) # 교집합\n",
    "    union = torch.sum(pred_1h + true_1h, dims) - inter # 합집합\n",
    "    \n",
    "    # mIoU\n",
    "    m_iou = (inter / (union + eps)).mean()\n",
    "    \n",
    "    # Dice Score\n",
    "    dice = (2. * inter / (torch.sum(pred_1h + true_1h, dims) + eps)).mean()\n",
    "    \n",
    "    return m_iou.item(), pix_acc.item(), dice.item()\n",
    "\n",
    "\n",
    "def mask2rle(img):\n",
    "    '''\n",
    "    베이스라인 로직:\n",
    "    img: 3채널 컬러 이미지 (BGR)\n",
    "    img.T.flatten(): 이미지를 전치(Transpose) 후 1차원으로 폄\n",
    "    '''\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "print(\"Losses & Utils defined (Fixed Version)!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0a6e1a7c-5c5a-4963-a87a-4e7e75206f71",
    "_uuid": "8530a7a5-3c94-4842-bb3e-f56d1308e0fe",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "6066e0ad-08fd-4607-b198-b97b54e07e07",
    "_uuid": "a01b96e7-e931-46a9-aea3-8445fa91528f",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T07:00:31.980845Z",
     "iopub.status.busy": "2025-12-07T07:00:31.980496Z",
     "iopub.status.idle": "2025-12-07T07:00:32.005936Z",
     "shell.execute_reply": "2025-12-07T07:00:32.005175Z",
     "shell.execute_reply.started": "2025-12-07T07:00:31.980815Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All functions updated successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def visualize_predictions(model, val_dl, device, save_dir, epoch, num_samples=5):\n",
    "    \"\"\"\n",
    "    검증 데이터셋의 일부를 추론하여 RGB, NIR, Pred, GT를 시각화하고 저장합니다.\n",
    "    기본적으로 5세트를 출력합니다.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 정규화 역변환 상수\n",
    "    MEAN = np.array([0.485, 0.456, 0.406])\n",
    "    STD = np.array([0.229, 0.224, 0.225])\n",
    "    NIR_MEAN = 0.5\n",
    "    NIR_STD = 0.25\n",
    "\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in val_dl:\n",
    "            rgb, nir = imgs\n",
    "            rgb, nir = rgb.to(device), nir.to(device)\n",
    "\n",
    "            if targets.dim() == 4:\n",
    "                targets = targets.squeeze(1)\n",
    "            gt_batch = targets.long().cpu().numpy()\n",
    "\n",
    "            preds = model(rgb, nir)\n",
    "            # 해상도 복원\n",
    "            if preds.shape[-2:] != targets.shape[-2:]:\n",
    "                preds = F.interpolate(preds, size=targets.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            pred_batch = preds.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "            # 배치 내 이미지 순회\n",
    "            for i in range(rgb.shape[0]):\n",
    "                if count >= num_samples:\n",
    "                    return # 목표 샘플 수(5개) 채우면 종료\n",
    "\n",
    "                # -----------------------------\n",
    "                # 1. 이미지 복원\n",
    "                # -----------------------------\n",
    "                img_rgb = rgb[i].permute(1, 2, 0).cpu().numpy()\n",
    "                img_rgb = img_rgb * STD + MEAN\n",
    "                img_rgb = np.clip(img_rgb, 0, 1)\n",
    "\n",
    "                img_nir = nir[i].squeeze().cpu().numpy()\n",
    "                img_nir = img_nir * NIR_STD + NIR_MEAN\n",
    "                img_nir = np.clip(img_nir, 0, 1)\n",
    "\n",
    "                img_gt = gt_batch[i]\n",
    "                img_pred = pred_batch[i]\n",
    "\n",
    "                # -----------------------------\n",
    "                # 2. Plotting (5장 모두 출력)\n",
    "                # -----------------------------\n",
    "                fig, ax = plt.subplots(1, 4, figsize=(16, 4))\n",
    "                \n",
    "                ax[0].imshow(img_rgb)\n",
    "                ax[0].set_title(f\"RGB\")\n",
    "                ax[0].axis(\"off\")\n",
    "\n",
    "                ax[1].imshow(img_nir, cmap='gray')\n",
    "                ax[1].set_title(f\"NIR\")\n",
    "                ax[1].axis(\"off\")\n",
    "\n",
    "                ax[2].imshow(img_pred, cmap='jet', vmin=0, vmax=3)\n",
    "                ax[2].set_title(f\"Prediction\")\n",
    "                ax[2].axis(\"off\")\n",
    "\n",
    "                ax[3].imshow(img_gt, cmap='jet', vmin=0, vmax=3)\n",
    "                ax[3].set_title(f\"GT Mask\")\n",
    "                ax[3].axis(\"off\")\n",
    "\n",
    "                plt.suptitle(f\"Epoch {epoch} - Sample {count+1}\", fontsize=14)\n",
    "                plt.tight_layout()\n",
    "\n",
    "                save_name = os.path.join(save_dir, f\"epoch_{epoch}_sample_{count}.png\")\n",
    "                plt.savefig(save_name)\n",
    "                \n",
    "                plt.show()\n",
    "                \n",
    "                count += 1\n",
    "\n",
    "def train_one_epoch(model, optimizer, data_loader, loss_fn, device, epoch, num_epochs, accumulation_steps=1):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pbar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f'Epoch {epoch}/{num_epochs}')\n",
    "    \n",
    "    for i, (imgs, targets) in pbar:\n",
    "        rgb, nir = imgs \n",
    "        rgb, nir, targets = rgb.to(device), nir.to(device), targets.to(device)\n",
    "        \n",
    "        if targets.dim() == 4:\n",
    "            targets = targets.squeeze(1)\n",
    "        targets = targets.long()\n",
    "        \n",
    "        preds = model(rgb, nir)\n",
    "\n",
    "        # 해상도 맞추기\n",
    "        if preds.shape[2:] != targets.shape[1:]:\n",
    "            preds = F.interpolate(preds, size=targets.shape[1:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        loss = loss_fn(preds, targets)\n",
    "        \n",
    "        # --- Gradient Accumulation ---\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        # 🔥 여기서 실제로 파라미터 업데이트\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(data_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        # -----------------------------\n",
    "\n",
    "        current_loss = loss.item() * accumulation_steps\n",
    "        losses.append(current_loss)\n",
    "        pbar.set_postfix({'loss': f'{current_loss:.4f}'})\n",
    "    \n",
    "    return np.mean(losses)\n",
    "\n",
    "def val_one_epoch(model, data_loader, device, epoch, num_epochs):\n",
    "    model.eval()\n",
    "    metrics = {'miou': [], 'acc': [], 'dice': []}\n",
    "    pbar = tqdm(data_loader, desc=f'Val {epoch}/{num_epochs}')\n",
    "    \n",
    "    for imgs, targets in pbar:\n",
    "        rgb, nir = imgs\n",
    "        rgb, nir, targets = rgb.to(device), nir.to(device), targets.to(device)\n",
    "        \n",
    "        if targets.dim() == 4: targets = targets.squeeze(1)\n",
    "        targets = targets.long()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            preds = model(rgb, nir)\n",
    "            if preds.shape[2:] != targets.shape[1:]:\n",
    "                preds = F.interpolate(preds, size=targets.shape[1:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            try:\n",
    "                m_iou, pix_acc, dice = fitness_test(targets.unsqueeze(1), preds)\n",
    "                metrics['miou'].append(m_iou)\n",
    "                metrics['acc'].append(pix_acc)\n",
    "                metrics['dice'].append(dice)\n",
    "                pbar.set_postfix({'mIOU': f'{m_iou:.4f}', 'Acc': f'{pix_acc:.4f}'})\n",
    "            except NameError:\n",
    "                pbar.set_postfix({'Status': 'Metric func missing'})\n",
    "\n",
    "    if len(metrics['miou']) == 0: return {'mIOU': 0, 'Accuracy': 0, 'Dice': 0}\n",
    "\n",
    "    return {\n",
    "        'mIOU': np.mean(metrics['miou']), \n",
    "        'Accuracy': np.mean(metrics['acc']), \n",
    "        'Dice': np.mean(metrics['dice'])\n",
    "    }\n",
    "\n",
    "import gc\n",
    "\n",
    "\n",
    "\n",
    "def train(model, optimizer, train_dl, val_dl, loss_func, epochs, device, \n",
    "          use_scheduler=False, save_path='./ckpt', scheduler_type='cosine', \n",
    "          val_interval=10, accumulation_steps=1): # [수정] 인자 추가\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    loss_fn = get_loss_function(loss_func)\n",
    "    \n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    visual_save_dir = os.path.join(save_path, \"visuals\")\n",
    "    \n",
    "    scheduler = None\n",
    "    if use_scheduler:\n",
    "        if scheduler_type == \"cosine\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)\n",
    "        elif scheduler_type == \"plateau\":\n",
    "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    \n",
    "    best_fit = 0.0\n",
    "    weight_file = os.path.join(save_path, 'cmx_best.pt')\n",
    "\n",
    "    print(f\"Start Training... Total Epochs: {epochs} (Val every {val_interval})\")\n",
    "    print(f\"Gradient Accumulation: {accumulation_steps} steps (Effective Batch: {train_dl.batch_size * accumulation_steps})\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # 1. Train\n",
    "        # [수정] accumulation_steps를 train_one_epoch에 전달\n",
    "        train_loss = train_one_epoch(model, optimizer, train_dl, loss_fn, device, epoch, epochs, accumulation_steps)\n",
    "        \n",
    "        # Scheduler\n",
    "        if scheduler:\n",
    "            if scheduler_type == \"plateau\":\n",
    "                scheduler.step(train_loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"\\n[Epoch {epoch}] Loss: {train_loss:.4f} | LR: {current_lr:.2e}\")\n",
    "\n",
    "        # 2. Validation (Interval)\n",
    "        if (epoch + 1) % val_interval == 0 or (epoch + 1) == epochs:\n",
    "            print(f\"    >>> Running Validation at Epoch {epoch}...\")\n",
    "            val_metrics = val_one_epoch(model, val_dl, device, epoch, epochs)\n",
    "            \n",
    "            print(f\"    Val mIOU: {val_metrics['mIOU']:.4f} | Acc: {val_metrics['Accuracy']:.4f}\")\n",
    "\n",
    "            # 5개 샘플 시각화 호출\n",
    "            visualize_predictions(model, val_dl, device, visual_save_dir, epoch, num_samples=5)\n",
    "\n",
    "            # Best Model Save\n",
    "            current_score = val_metrics['mIOU']\n",
    "            if current_score > best_fit:\n",
    "                best_fit = current_score\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'best_score': val_metrics, # 딕셔너리 저장 (Load 에러 방지)\n",
    "                }, weight_file)\n",
    "                print(f\"    >>> Best Model Saved! (mIOU: {best_fit:.4f})\")\n",
    "        \n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    print(f\"Training Completed. Best mIOU: {best_fit:.4f}\")\n",
    "    return best_fit\n",
    "print(\"All functions updated successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "afc47656-3c62-47dd-8649-ce8a5048c50b",
    "_uuid": "36dd787a-ad66-4686-8540-6970b62df164",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 9. Testing & Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "b9163bd9-64e8-4298-85c6-29042714120e",
    "_uuid": "b0148826-189f-4012-97b2-e2f0fdc8c253",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:01.137680Z",
     "iopub.status.busy": "2025-12-07T06:55:01.137128Z",
     "iopub.status.idle": "2025-12-07T06:55:01.150881Z",
     "shell.execute_reply": "2025-12-07T06:55:01.150124Z",
     "shell.execute_reply.started": "2025-12-07T06:55:01.137657Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_best_model(model, device, output_path):\n",
    "    \"\"\"최고 성능 모델 로드 (CMX 전용)\"\"\"\n",
    "    save_path = os.path.join(output_path, 'ckpt')\n",
    "    checkpoint_path = os.path.join(save_path, 'cmx_best.pt')\n",
    "    \n",
    "    if not os.path.exists(checkpoint_path):\n",
    "        print(f\"Warning: Model not found at {checkpoint_path}\")\n",
    "        return model\n",
    "        \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.to(device)\n",
    "    print('Model load success')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b731dc17-94c2-4a39-a093-4afc0cd16fc3",
    "_kg_hide-input": true,
    "_uuid": "50c329ed-2bea-4b96-9311-7e2a76841ccc",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:01.153308Z",
     "iopub.status.busy": "2025-12-07T06:55:01.153065Z",
     "iopub.status.idle": "2025-12-07T06:55:01.165629Z",
     "shell.execute_reply": "2025-12-07T06:55:01.165077Z",
     "shell.execute_reply.started": "2025-12-07T06:55:01.153293Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing functions defined!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def predict_test_set(model, test_dataloader, device, result_path):\n",
    "    model.eval()\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "    \n",
    "    print(f\"Generating predictions to {result_path}...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # CMX Dataset은 ((rgb, nir), path)를 반환함\n",
    "        for batch_idx, (imgs, img_paths) in tqdm(enumerate(test_dataloader), total=len(test_dataloader)):\n",
    "            rgb, nir = imgs\n",
    "            rgb, nir = rgb.to(device), nir.to(device)\n",
    "            \n",
    "            # 1. 모델 추론\n",
    "            preds = model(rgb, nir)\n",
    "            \n",
    "            # 2. 원본 크기 복원 (베이스라인은 원본 크기로 저장함)\n",
    "            # 이미지 경로에서 원본 이미지 크기를 읽어와서 맞춤\n",
    "            original_img = cv2.imread(img_paths[0])\n",
    "            h_orig, w_orig = original_img.shape[:2]\n",
    "            \n",
    "            if preds.shape[2:] != (h_orig, w_orig):\n",
    "                preds = F.interpolate(preds, size=(h_orig, w_orig), mode='bilinear', align_corners=False)\n",
    "            \n",
    "            # 3. Argmax로 클래스 인덱스 추출 (Batch, H, W)\n",
    "            _, idx_mask = preds.max(1) \n",
    "            \n",
    "            # 4. 배치 내 이미지 처리 (Batch Size가 1이라도 루프 유지)\n",
    "            for i in range(rgb.shape[0]):\n",
    "                # 빈 컬러 이미지 생성 (H, W, 3)\n",
    "                pred_img = np.zeros((h_orig, w_orig, 3), dtype=np.uint8)\n",
    "                \n",
    "                # GPU 텐서를 numpy로 변환\n",
    "                mask = idx_mask[i].cpu().numpy()\n",
    "                \n",
    "                # [핵심] 베이스라인과 동일한 BGR 색상 매핑\n",
    "                # 0: Background (Black) - [0, 0, 0] (이미 0으로 초기화됨)\n",
    "                pred_img[mask == 1] = [0, 0, 255]    # Thick Cloud (Red)\n",
    "                pred_img[mask == 2] = [0, 255, 0]    # Thin Cloud (Green)\n",
    "                pred_img[mask == 3] = [0, 255, 255]  # Cloud Shadow (Yellow)\n",
    "                \n",
    "                # 5. 이미지 저장\n",
    "                filename = os.path.basename(img_paths[i])\n",
    "                cv2.imwrite(os.path.join(result_path, filename), pred_img)\n",
    "def create_submission(result_path, workspace_path):\n",
    "    print(\"Creating submission.csv...\")\n",
    "    \n",
    "    # 결과 폴더의 파일 리스트 (정렬 필수)\n",
    "    test_label_file_list = sorted(os.listdir(result_path))\n",
    "    test_label_path_list = [os.path.join(result_path, x) for x in test_label_file_list]\n",
    "    \n",
    "    rle_list = []\n",
    "    # 이미지를 하나씩 읽어서 RLE 인코딩\n",
    "    for file_path in tqdm(test_label_path_list, desc=\"RLE Encoding\"):\n",
    "        img = cv2.imread(file_path) # BGR로 읽기\n",
    "        rle = mask2rle(img)         # 위에서 정의한 mask2rle 함수 사용\n",
    "        rle_list.append(rle)\n",
    "    \n",
    "    # DataFrame 생성\n",
    "    submission = pd.DataFrame({\n",
    "        'Image_Label': test_label_file_list,\n",
    "        'EncodedPixels': rle_list\n",
    "    })\n",
    "    \n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    print(\"submission.csv saved successfully!\")\n",
    "    print(submission.head())\n",
    "    return submission\n",
    "    \n",
    "print(\"Testing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a0cb052a-92a8-45ce-b61e-96327a085d78",
    "_uuid": "027d72a9-86f4-4e23-a5b9-e15e185808af",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 10. Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "c7a53e00-fb43-4059-bb9a-de7a300b329f",
    "_uuid": "d0994367-2dc1-402c-94d2-5f2d988466fb",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:01.166528Z",
     "iopub.status.busy": "2025-12-07T06:55:01.166314Z",
     "iopub.status.idle": "2025-12-07T06:55:01.187652Z",
     "shell.execute_reply": "2025-12-07T06:55:01.187060Z",
     "shell.execute_reply.started": "2025-12-07T06:55:01.166504Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main functions updated for new CloudDataset!\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"데이터 경로 로드\"\"\"\n",
    "\n",
    "    rgb_path = os.path.join(workspace_path, 'train/rgb/')\n",
    "    ngr_path = os.path.join(workspace_path, 'train/ngr/')\n",
    "    label_path = os.path.join(workspace_path, 'train/label/')\n",
    "    \n",
    "    rgb_images = sorted([os.path.join(rgb_path, x) for x in os.listdir(rgb_path)])\n",
    "    ngr_images = sorted([os.path.join(ngr_path, x) for x in os.listdir(ngr_path)])\n",
    "    label_images = sorted([os.path.join(label_path, x) for x in os.listdir(label_path)])\n",
    "    \n",
    "    return rgb_images, ngr_images, label_images\n",
    "\n",
    "\n",
    "def create_dataloaders(rgb_images, ngr_images, label_images):\n",
    "    \"\"\"데이터로더 생성 (New CloudDataset 호환)\"\"\"\n",
    "    train_split = int(len(rgb_images) * train_data_rate)\n",
    "    \n",
    "    # 1. Train Dataset\n",
    "    # patch_size -> crop_size로 이름 변경됨\n",
    "    # transforms 객체를 밖에서 넣지 않고 내부에서 생성함\n",
    "    train_ds = CloudDataset(\n",
    "        rgb_paths=rgb_images[:train_split], \n",
    "        ngr_paths=ngr_images[:train_split], \n",
    "        label_paths=label_images[:train_split],\n",
    "        is_train=True, \n",
    "        crop_size=patch_size, \n",
    "        use_copy_paste=use_copy_paste # 전역 변수 or 인자 활용\n",
    "    )\n",
    "    \n",
    "    # 2. Val Dataset\n",
    "    val_ds = CloudDataset(\n",
    "        rgb_paths=rgb_images[train_split:], \n",
    "        ngr_paths=ngr_images[train_split:], \n",
    "        label_paths=label_images[train_split:],\n",
    "        is_train=False\n",
    "        # crop_size, use_copy_paste 필요 없음\n",
    "    )\n",
    "    \n",
    "    # 3. DataLoaders\n",
    "    # Validation은 batch_size=1을 권장 (검증 시 원본 크기 유지 등 이슈 최소화)\n",
    "    train_dl = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=batch_size, shuffle=True, \n",
    "        num_workers=num_workers, pin_memory=True, drop_last=True\n",
    "    )\n",
    "    val_dl = torch.utils.data.DataLoader(\n",
    "        val_ds, batch_size=1, shuffle=False, \n",
    "        num_workers=num_workers, pin_memory=True, drop_last=True\n",
    "    )\n",
    "    \n",
    "    return train_dl, val_dl\n",
    "\n",
    "\n",
    "def create_test_dataloader():\n",
    "    \"\"\"테스트 데이터로더 (New CloudDataset 호환)\"\"\"\n",
    "    # 테스트 경로 설정\n",
    "    test_rgb_dir = os.path.join(workspace_path, 'test/rgb')\n",
    "    test_ngr_dir = os.path.join(workspace_path, 'test/ngr')\n",
    "    \n",
    "    test_rgb = sorted([os.path.join(test_rgb_dir, x) for x in os.listdir(test_rgb_dir)])\n",
    "    test_ngr = sorted([os.path.join(test_ngr_dir, x) for x in os.listdir(test_ngr_dir)])\n",
    "    \n",
    "    # 주의: CloudDataset은 label_paths가 필수이므로, 더미 라벨(또는 빈 리스트) 처리 필요\n",
    "    # 현재 CloudDataset 구조상 빈 리스트 []를 넣으면 __getitem__에서 에러날 수 있음.\n",
    "    # 테스트용이라 라벨이 없다면, 아래처럼 Dataset 클래스를 살짝 수정하거나 \n",
    "    # 더미 경로를 넣어줘야 하는데, 일단은 빈 리스트로 호출해봅니다.\n",
    "    # (만약 에러나면 Test 전용 Dataset 클래스를 분리해야 함)\n",
    "    \n",
    "    test_ds = CloudDataset(\n",
    "        rgb_paths=test_rgb, \n",
    "        ngr_paths=test_ngr, \n",
    "        label_paths=[], # 라벨 없음\n",
    "        is_train=False\n",
    "    )\n",
    "    \n",
    "    return torch.utils.data.DataLoader(\n",
    "        test_ds, batch_size=1, shuffle=False, \n",
    "        num_workers=num_workers, pin_memory=True\n",
    "    )\n",
    "\n",
    "print(\"Main functions updated for new CloudDataset!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0f3dcaff-1e4e-43f7-a2ea-1c85c7601844",
    "_uuid": "c6045d26-af3f-478a-b963-eb3d79ff37d2",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## 11. 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b77f346b-eac0-4009-b99c-efac1fcfb88f",
    "_uuid": "f50eb101-200e-4ed8-a59e-a38022585e39",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 11.1 데이터 로드 및 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "9a5686af-baa7-4c8b-9706-621aa19dd626",
    "_uuid": "667a487e-b196-45f8-884e-adb419563ae9",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T06:55:01.188699Z",
     "iopub.status.busy": "2025-12-07T06:55:01.188393Z",
     "iopub.status.idle": "2025-12-07T06:56:12.632348Z",
     "shell.execute_reply": "2025-12-07T06:56:12.631547Z",
     "shell.execute_reply.started": "2025-12-07T06:55:01.188677Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "  RGB: 829, NGR: 829, Label: 829\n",
      "\n",
      "Creating dataloaders...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 165, Val batches: 166\n",
      "\n",
      "Creating model...\n",
      "  Backbone: mit_b2\n",
      "  Parameters: 66,562,956\n",
      "  Optimizer: AdamW initialized with separated LR\n",
      "   - Backbone LR: 3.0e-05\n",
      "   - Head LR:     3.0e-04\n"
     ]
    }
   ],
   "source": [
    "# 시드 초기화\n",
    "init_seeds(seed)\n",
    "\n",
    "# 데이터 로드\n",
    "print(\"Loading data...\")\n",
    "rgb_images, ngr_images, label_images = load_data()\n",
    "print(f\"  RGB: {len(rgb_images)}, NGR: {len(ngr_images)}, Label: {len(label_images)}\")\n",
    "\n",
    "# 데이터로더 생성\n",
    "print(\"\\nCreating dataloaders...\")\n",
    "train_dl, val_dl = create_dataloaders(rgb_images, ngr_images, label_images)\n",
    "print(f\"Train batches: {len(train_dl)}, Val batches: {len(val_dl)}\")\n",
    "\n",
    "# 모델 생성\n",
    "print(\"\\nCreating model...\")\n",
    "model = create_model(device, num_classes, cmx_backbone, cmx_pretrained, use_timm_pretrained)\n",
    "print(f\"  Backbone: {cmx_backbone}\")\n",
    "print(f\"  Parameters: {count_parameters(model):,}\")\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW([\n",
    "    {\"params\": model.backbone.parameters(), \"lr\": lr_backbone},\n",
    "    {\"params\": model.decode_head.parameters(), \"lr\": lr_head},\n",
    "], weight_decay=weight_decay)\n",
    "\n",
    "print(f\"  Optimizer: AdamW initialized with separated LR\")\n",
    "print(f\"   - Backbone LR: {lr_backbone:.1e}\")\n",
    "print(f\"   - Head LR:     {lr_head:.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "cd4edb7e-b32e-4ead-9444-6bd37185efb7",
    "_uuid": "465ae383-da22-4f54-a749-adc30a95603f",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 11.2 학습 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "fd4b5f62-4b06-4491-8cb9-1c8f3ff7edc4",
    "_uuid": "1d08c53f-46ea-483f-8fc4-de68457dd6e4",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T07:01:12.902517Z",
     "iopub.status.busy": "2025-12-07T07:01:12.901972Z",
     "iopub.status.idle": "2025-12-07T18:15:19.997548Z",
     "shell.execute_reply": "2025-12-07T18:15:19.996759Z",
     "shell.execute_reply.started": "2025-12-07T07:01:12.902494Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "Start Training... Total Epochs: 60 (Val every 5)\n",
      "Gradient Accumulation: 4 steps (Effective Batch: 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0/60:   5%|▌         | 9/165 [00:26<07:42,  2.97s/it, loss=1.9491]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 학습 시작\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m best_metrics = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_scheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_scheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mckpt\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\n\u001b[32m     15\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 193\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, optimizer, train_dl, val_dl, loss_func, epochs, device, use_scheduler, save_path, scheduler_type, val_interval, accumulation_steps)\u001b[39m\n\u001b[32m    188\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGradient Accumulation: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccumulation_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m steps (Effective Batch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_dl.batch_size\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39maccumulation_steps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    190\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m    191\u001b[39m     \u001b[38;5;66;03m# 1. Train\u001b[39;00m\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# [수정] accumulation_steps를 train_one_epoch에 전달\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulation_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;66;03m# Scheduler\u001b[39;00m\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m scheduler:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 115\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, optimizer, data_loader, loss_fn, device, epoch, num_epochs, accumulation_steps)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# --- Gradient Accumulation ---\u001b[39;00m\n\u001b[32m    114\u001b[39m loss = loss / accumulation_steps\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# 🔥 여기서 실제로 파라미터 업데이트\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i + \u001b[32m1\u001b[39m) % accumulation_steps == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m (i + \u001b[32m1\u001b[39m) == \u001b[38;5;28mlen\u001b[39m(data_loader):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 학습 시작\n",
    "print(\"\\nStarting training...\")\n",
    "best_metrics = train(\n",
    "    model=model, \n",
    "    optimizer=optimizer, \n",
    "    train_dl=train_dl, \n",
    "    val_dl=val_dl, \n",
    "    loss_func=loss_func, \n",
    "    epochs=epochs, \n",
    "    device=device,\n",
    "    use_scheduler=use_scheduler,\n",
    "    save_path=os.path.join(output_path, 'ckpt'),\n",
    "    val_interval=5,\n",
    "    accumulation_steps=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "567c48ab-d464-4ac5-998d-9d1dae22f9df",
    "_uuid": "7cc50b17-e450-4f48-b9f4-3efe94f574e1",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T18:15:19.999995Z",
     "iopub.status.busy": "2025-12-07T18:15:19.999653Z",
     "iopub.status.idle": "2025-12-07T18:15:20.005327Z",
     "shell.execute_reply": "2025-12-07T18:15:20.004570Z",
     "shell.execute_reply.started": "2025-12-07T18:15:19.999970Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"\\nBest Results:\")\n",
    "if isinstance(best_metrics, dict):\n",
    "    print(f\"  mIOU: {best_metrics.get('mIOU', 0):.4f}\")\n",
    "    print(f\"  Accuracy: {best_metrics.get('Accuracy', 0):.4f}\")\n",
    "    print(f\"  Dice: {best_metrics.get('Dice', 0):.4f}\")\n",
    "else:\n",
    "\n",
    "    print(f\"  mIOU (Main Score): {best_metrics:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b185a76f-6589-4b87-bfa8-34bfba3ceffc",
    "_uuid": "6bc583df-78fb-49e1-bcbe-a022e9930667",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "### 11.3 테스트 및 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3b8ca1f4-42dc-4747-81d2-153e6cd0a260",
    "_uuid": "58020943-2ac8-4677-9f31-142ae0968112",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-07T18:15:20.006224Z",
     "iopub.status.busy": "2025-12-07T18:15:20.006033Z",
     "iopub.status.idle": "2025-12-07T18:16:42.200790Z",
     "shell.execute_reply": "2025-12-07T18:16:42.200065Z",
     "shell.execute_reply.started": "2025-12-07T18:15:20.006209Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 베스트 모델 로드\n",
    "model = load_best_model(model, device, output_path)\n",
    "\n",
    "# 테스트 데이터로더\n",
    "test_dl = create_test_dataloader()\n",
    "\n",
    "# 예측\n",
    "result_path = os.path.join(output_path, 'results')\n",
    "predict_test_set(model, test_dl, device, result_path)\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission = create_submission(result_path, workspace_path)\n",
    "print(submission.head())\n",
    "print(\"\\nDone!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14515056,
     "sourceId": 121314,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
